= Deploy a sample pod

Now that the everything is ready, we can run a sample workload.
Let's first see what we can and must add into the pod yaml to make it run in a VM.

NOTE: All **yaml** files in this section could be also created in the web UI by clicking on the **Import yaml** button in the top bar on the right (circled in red in the image below). Importing a yaml in this way corresponds to creating a `.yaml` file and applying it with `oc apply -f`. Because we are going to perform also other command-line operations, like fetching ARO credentials, we will perform all operations via command line. Note that this does not apply for `az` or `bash` (`cat`, `oc`) commands

image::05-bar.png[link=self, window=blank]

[#options]
== Available options

=== Mandatory options
In order to run a pod in a VM, it is mandatory to specify the `runtimeClassName` field in the pod `spec`. For peer-pods, the runtime class is called `kata-remote`.

[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
# ...
spec:
  runtimeClassName: kata-remote
# ...
----

=== Optionals

* Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: Standard_D32as_v5
# ...
----
+
Note that the `machine_type` must be one of the one specified in `AZURE_INSTANCE_SIZES` or `AZURE_INSTANCE_SIZE` in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap]. By default, all instance types will be `AZURE_INSTANCE_SIZE`.

* Define the amount of memory available for the workload to use. The workload will run on an automatic instance size based on the amount of memory available.
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: <vcpus>
    io.katacontainers.config.hypervisor.default_memory: <memory>
# ...
----

[#example]
== Hello world example
This is a sample yaml that runs an `hello-openshift` pod. This pod acts as server and outputs `"Hello Openshift!"` every time is reached.

. Create and apply the yaml file.
+
[source,sh,role=execute]
----
cat > sample-openshift.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: hello-openshift
  labels:
    app: hello-openshift
spec:
  runtimeClassName: kata-remote
  containers:
    - name: hello-openshift
      image: quay.io/openshift/origin-hello-openshift
      ports:
        - containerPort: 8888
      securityContext:
        privileged: false
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 1001
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
---
kind: Service
apiVersion: v1
metadata:
  name: hello-openshift-service
  labels:
    app: hello-openshift
spec:
  selector:
    app: hello-openshift
  ports:
    - port: 8888
EOF

cat sample-openshift.yaml
----
+
[source,sh,role=execute]
----
oc apply -f sample-openshift.yaml
----

. Wait that the pod is created.
+
[source,sh,role=execute]
----
watch oc get pods/hello-openshift
----
+
The pod is ready when the `STATUS` is in `Running`.

. Now expose the pod to make it reachable:
+
[source,sh,role=execute]
----
oc expose service hello-openshift-service -l app=hello-openshift
APP_URL=$(oc get routes/hello-openshift-service -o jsonpath='{.spec.host}')
----

. And try to connect to the pod. It should print `Hello Openshift!`.
+
[source,sh,role=execute]
----
curl ${APP_URL}
----

[#verify]
== Verify that the pod is running in a VM
How to be sure that all what we did so far is actually running in a VM? There are several ways to check this.

. Via ARO web UI.
  * Go to the https://portal.azure.com/#browse/Microsoft.Compute%2FVirtualMachines/subscriptionId/{azure_subscription}[Azure VM web console, window=blank] and login. Insert the email associated with your RHDP account and proceed with login.
  * In the `Subscription` filter, make sure {aro_sandbox_name} is selected. In this example, it is `pool-01-344`.
+
image::06-subscription.png[link=self, window=blank]
  * Look at the various VMs. You will see there are:
    ** 3 masters VM (called _aro-cluster-{guid}-<random chars>-master-0/1/2_)
    ** 3 workers VM (called _aro-cluster-{guid}-<random chars>-worker-<region>-<random chars>_)
    ** 1 _bastion-{guid}_ VM, used internally by the workshop infrastructure. The console on the right is actually connected to this VM, and all commands are being performed from here.
    ** 1 **podvm-hello-openshift-<random chars>**. This is where the `hello-openshift` pod is actually running! Note also how the instance tyoe under `Size` column at the right side is not the same as the other VMs. It is indeed `Standard_D8as_v5`, as specified in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap].
+
image::07-hello.png[link=self, window=blank]

. Via command line using `az`. Result and observations are same as the web UI.
+
[source,sh,role=execute]
----
az vm list --query "[].{Name:name, VMSize:hardwareProfile.vmSize}" --output table
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
Name                                          VMSize
--------------------------------------------  ----------------
aro-cluster-q5hqf-xs7zb-master-0              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-1              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-2              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus2-vt87j  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus3-6dzt4  Standard_D4s_v3
podvm-hello-openshift-c0311387                Standard_D8as_v5
bastion-q5hqf                                 Standard_DS1_v2
----

. By SSH'ing into the VM.
  * Recall `id_rsa` created when xref:02-configure-osc.adoc#pp-key[setting up the operator]. We will use that to log into the pod VM. If you followed all the commands so far, your key should be in the folder where you currently are.
  * Get the pod VM private ip address:
    ** List all VMs to find your pod VM name. For our hello world example, it should be something like `podvm-hello-openshift-<random chars>`:
+
[source,sh,role=execute]
----
ARO_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}')

az vm list \
  --resource-group $ARO_RESOURCE_GROUP \
  --output table
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
Name                                          ResourceGroup    Location    Zones
--------------------------------------------  ---------------  ----------  -------
aro-cluster-q5hqf-xs7zb-master-0              aro-gqvj3nvq     eastus      1
aro-cluster-q5hqf-xs7zb-master-1              aro-gqvj3nvq     eastus      2
aro-cluster-q5hqf-xs7zb-master-2              aro-gqvj3nvq     eastus      3
aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl  aro-gqvj3nvq     eastus      1
aro-cluster-q5hqf-xs7zb-worker-eastus2-vt87j  aro-gqvj3nvq     eastus      2
aro-cluster-q5hqf-xs7zb-worker-eastus3-6dzt4  aro-gqvj3nvq     eastus      3
podvm-hello-openshift-c0311387                aro-gqvj3nvq     eastus
----
    ** Get private ip of the **podvm-hello-openshift-<random_char>** VM (in this case, `podvm-hello-openshift-c0311387`):
+
[source,sh,role=execute]
----
VM_NAME=podvm-hello-openshift-c0311387 # replace it with your pod vm name!
ARO_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}')

az vm list-ip-addresses --name $VM_NAME \
  --resource-group $ARO_RESOURCE_GROUP \
  --output table
----
+
Example output, the ip will probably be different for you:
+
[source,texinfo,subs="attributes"]
----
VirtualMachine                  PrivateIPAddresses
------------------------------  --------------------
podvm-hello-openshift-c0311387  10.0.2.10
----
  * Log into one of the CAA pods running in the worker nodes. These pods are part the OSC operator and take care of creating, handling and destroying all the peer pods containers and VMs:
+
[source,sh,role=execute]
----
# in the bastion node:
oc exec -it -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon -- bash
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
[azure@bastion-q5hqf ~]$ # <--- Notice how we are now in the bastion node
[azure@bastion-q5hqf ~]$ oc exec -it -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon -- bash
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /]# # <--- Notice how we are now in a pod running in a worker node!
----
  * Once inside the CAA pod, the `id_rsa` key to access the pod is already embedded inside in `/root/.ssh/`. Therefore from the CAA pod, simply ssh into the pod VM using the IP that you just found. In our case, it's `10.0.2.10`. By default, a peer pod VM has always an user called `peerpod`. Let's log in with that:
+
[source,sh,role=execute]
----
# in the CAA pod:
PEER_POD_VM_IP=10.0.2.10 # replace it with your pod vm ip!
ssh peerpod@$PEER_POD_VM_IP
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /]# ssh peerpod@10.0.2.10 # <--- Notice how we are now in a pod running in a worker node!
[...]
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
[...]
[peerpod@podvm-hello-openshift-c0311387 ~]$ # <--- Notice how we are now in the pod VM!
----
  * Inspect the VM. For example, check the ip address with `ip addr` and see how it differs from the original VM, check the kernel version with `uname -r`, processes running with `ps -aux`, os-release with `cat /etc/os-release` and so on.
+
Example: check the kernel version in both a normal pod running in the worker node, and the pod VM.
  ** If you still are ssh'ed in the pod VM, exit from it using `exit` or pressing the key combination Ctrl+d:
+
[source,texinfo,subs="attributes"]
----
[peerpod@podvm-hello-openshift-c0311387 ~]$ exit # <--- Notice how we are now in the pod VM!
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /]# # <--- Notice how we are now in a pod running in a worker node!
----
  ** Run `uname -r` in the CAA pod running in the worker node. Notice which kernel it prints.
+
[source,sh,role=execute]
----
# in the CAA pod:
uname -r # get current worker node kernel
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
# in the CAA pod:
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /] uname -r # get current worker node kernel
4.18.0-372.64.1.el8_6.x86_64 # <----------------
----
  ** Run `uname -r` in the pod VM. Notice which kernel it prints, and how it differs from the worker node.
+
[source,sh,role=execute]
----
# in the CAA pod:
ssh peerpod@$PEER_POD_VM_IP uname -r # get pod VM kernel
----
+
Example output. The worker node kernel above is `4.18.0-372.64.1.el8_6.x86_64` while the pod VM is `5.14.0-362.8.1.el9_3.x86_64`:
+
[source,texinfo,subs="attributes"]
----
# in the CAA pod:
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /] ssh peerpod@$PEER_POD_VM_IP uname -r
[...]
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
[...]
5.14.0-362.8.1.el9_3.x86_64 # <----------------
----
** Now, exit from the CAA pod by typing either `exit` or pressing the key combination Ctrl+d.
+
[source,texinfo,subs="attributes"]
----
[root@aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl /]# exit # <--- Notice how we are now in a pod running in a worker node!
[azure@bastion-q5hqf ~]$ # <--- back to the bastion node!
----
** To confirm the kernel difference, run another test pod without using the `kata-remote` runtimeclass. This pod, just like all the traditional ones, is also expected to run in the worker node.
+
[source,sh,role=execute]
----
oc apply -f-<<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test
  labels:
    app: test
spec:
  containers:
    - name: test
      image: quay.io/openshift/origin-hello-openshift
      ports:
        - containerPort: 8888
      securityContext:
        privileged: false
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 1001
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
EOF
----

** Enter inside the newly created `test` pod and confirm that it is running on the same worker node kernel, `4.18.0-372.64.1.el8_6.x86_64`, and therefore it differs from the pod VM kernel:
+
[source,sh,role=execute]
----
# in the bastion node:
oc exec pods/test -- uname -r
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
[azure@bastion-q5hqf ~]$ oc exec pods/test -- uname -r
4.18.0-372.64.1.el8_6.x86_64
----

[#destroy]
== Destroy the hello-openshift pod
The `hello-openshift` pod is no different from any other pod, therefore it can be destroyed just as the others (via command line, web ui, etc.). Behind the scenes, the operator will make sure that the created VM will also be completely deallocated.