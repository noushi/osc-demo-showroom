= Deploy a sample pod

Now that the everything is ready, we can run a sample workload.
Let's first see what we can and must add into the pod yaml to make it run in a VM.

NOTE: Because of CPU Quota limitations of this ARO subscription, and since each CoCo runs in a Confidential VM, you are just allowed to get one single CoCo at time. Trying to deploy a second one will result in failure and container stuck in `ContainerCreating` state.

[#options]
== Available options

=== Mandatory options
In order to run a pod in a VM, it is mandatory to specify the `runtimeClassName` field in the pod `spec`. For peer-pods, the runtime class is called `kata-remote`.

[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
# ...
spec:
  runtimeClassName: kata-remote
# ...
----

=== Optionals

* Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: Standard_D32as_v5
# ...
----
+
Note that the `machine_type` must be one of the one specified in `AZURE_INSTANCE_SIZES` or `AZURE_INSTANCE_SIZE` in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap]. By default, all instance types will be `AZURE_INSTANCE_SIZE`.

* Define the amount of memory available for the workload to use. The workload will run on an automatic instance size based on the amount of memory available.
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: <vcpus>
    io.katacontainers.config.hypervisor.default_memory: <memory>
# ...
----

* **NOT SUITABLE FOR PRODUCTION**: Enable pod logs/exec. A Confidential Container doesn't enable by default logging or exec into the pod, otherwise a malicious actor (oc admin) could be able to enter and see the secrets. For the purpose of this workshop, it is possible to enable logs using an https://github.com/kata-containers/kata-containers/blob/main/docs/how-to/how-to-use-the-kata-agent-policy.md#encode-a-policy-file[alternative Kata policy, window=blank].
+
A Kata policy is defined as `annotation` in the pod yaml, and it's base64 encoded. It defines what a CoCo pod is allowed to do and not. Such policies can be generated with the https://github.com/kata-containers/kata-containers/blob/main/src/tools/genpolicy/README.md[genpolicy, window=blank] tool.
+
For example the https://github.com/kata-containers/kata-containers/blob/main/src/kata-opa/allow-all-except-exec-process.rego[allow-all-except-exec-process.rego, window=blank] enables pod logs while keeping exec disabled.
+
The base64 encoded allow-all-except-exec-process.rego policy is the following:
+
[source,sh,role=execute]
----
io.katacontainers.config.agent.policy: cGFja2FnZSBhZ2VudF9wb2xpY3kKCmRlZmF1bHQgQWRkQVJQTmVpZ2hib3JzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgQWRkU3dhcFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENsb3NlU3RkaW5SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBDb3B5RmlsZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZVNhbmRib3hSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBEZXN0cm95U2FuZGJveFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEdldE1ldHJpY3NSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBHZXRPT01FdmVudFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEd1ZXN0RGV0YWlsc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IExpc3RJbnRlcmZhY2VzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTGlzdFJvdXRlc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IE1lbUhvdHBsdWdCeVByb2JlUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgT25saW5lQ1BVTWVtUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgUGF1c2VDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBQdWxsSW1hZ2VSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZWFkU3RyZWFtUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgUmVtb3ZlQ29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgUmVtb3ZlU3RhbGVWaXJ0aW9mc1NoYXJlTW91bnRzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgUmVzZWVkUmFuZG9tRGV2UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgUmVzdW1lQ29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2V0R3Vlc3REYXRlVGltZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFNldFBvbGljeVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFNpZ25hbFByb2Nlc3NSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTdGFydENvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFN0YXJ0VHJhY2luZ1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFN0YXRzQ29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RvcFRyYWNpbmdSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBUdHlXaW5SZXNpemVSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBVcGRhdGVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBVcGRhdGVFcGhlbWVyYWxNb3VudHNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBVcGRhdGVJbnRlcmZhY2VSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBVcGRhdGVSb3V0ZXNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBXYWl0UHJvY2Vzc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFdyaXRlU3RyZWFtUmVxdWVzdCA6PSB0cnVlCgpkZWZhdWx0IEV4ZWNQcm9jZXNzUmVxdWVzdCA6PSBmYWxzZQo=
----
+
And it can be provided to any CoCo pod by adding it as `metadata.annotations`:
+
[source,sh,role=execute]
----
apiVersion: v1
kind: Pod
metadata:
  name: policy-exec-rejected
  annotations:
    io.katacontainers.config.agent.policy: cGFja2....
...
----

[#example]
== Hello world example

In this example we will show how easy is to modify an existing pod to make it running in CoCo, i.e. specifying the `runtimeclass` in the podspec. No other action is necessary w.r.t the pod itself, and the confidential VM is completely transparent to it.

This is a sample yaml that runs an `hello-openshift` pod. The pod application is not developed by the CoCo team, nor was modified purposefully for this example. The code is available https://github.com/openshift-for-developers/hello[here, window=blank]. This pod creates a server and outputs `"Hello Openshift!"` every time is reached. The difference between this pod deployed as confidential container and traditional pod is just that the former has `spec.runtimeClassName: kata-remote` defined in the pod spec.

. Create and apply the yaml file.
+
[source,sh,role=execute]
----
cat > sample-openshift.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: hello-openshift
  labels:
    app: hello-openshift
spec:
  runtimeClassName: kata-remote
  containers:
    - name: hello-openshift
      image: quay.io/openshift/origin-hello-openshift
      ports:
        - containerPort: 8888
      securityContext:
        privileged: false
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 1001
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
---
kind: Service
apiVersion: v1
metadata:
  name: hello-openshift-service
  labels:
    app: hello-openshift
spec:
  selector:
    app: hello-openshift
  ports:
    - port: 8888
EOF

cat sample-openshift.yaml
----
+
[source,sh,role=execute]
----
oc apply -f sample-openshift.yaml
----

. Wait that the pod is created.
+
[source,sh,role=execute]
----
watch oc get pods/hello-openshift
----
+
The pod is ready when the `STATUS` is in `Running`.

. Now expose the pod to make it reachable:
+
[source,sh,role=execute]
----
oc expose service hello-openshift-service -l app=hello-openshift
APP_URL=$(oc get routes/hello-openshift-service -o jsonpath='{.spec.host}')
----

. And try to connect to the pod. It should print `Hello Openshift!`.
+
[source,sh,role=execute]
----
curl ${APP_URL}
----

[#verify]
== Verify that the pod is running in a VM
How to be sure that all what we did so far is actually running in a VM? There are several ways to check this.

. Via ARO web UI.
  * Go to the https://portal.azure.com/#browse/Microsoft.Compute%2FVirtualMachines/subscriptionId/{azure_subscription}[Azure VM web console, window=blank] and login. Insert the email associated with your RHDP account and proceed with login.
  * In the `Subscription` filter, make sure {aro_sandbox_name} is selected. In this example, it is `pool-01-344`.
+
image::06-subscription.png[link=self, window=blank]
  * Look at the various VMs. You will see there are:
    ** 3 masters VM (called _aro-cluster-{guid}-<random chars>-master-0/1/2_)
    ** 3 workers VM (called _aro-cluster-{guid}-<random chars>-worker-<region>-<random chars>_)
    ** 1 _bastion-{guid}_ VM, used internally by the workshop infrastructure. The console on the right is actually connected to this VM, and all commands are being performed from here.
    ** 1 **podvm-hello-openshift-<random chars>**. This is where the `hello-openshift` pod is actually running! Note also how the instance tyoe under `Size` column at the right side is not the same as the other VMs. It is indeed `Standard_D8as_v5`, as specified in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap].
+
image::07-hello.png[link=self, window=blank]

. Via command line using `az`. Result and observations are same as the web UI.
+
[source,sh,role=execute]
----
az vm list --query "[].{Name:name, VMSize:hardwareProfile.vmSize}" --output table
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
Name                                          VMSize
--------------------------------------------  ----------------
aro-cluster-q5hqf-xs7zb-master-0              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-1              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-2              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus2-vt87j  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus3-6dzt4  Standard_D4s_v3
podvm-hello-openshift-c0311387                Standard_D8as_v5
bastion-q5hqf                                 Standard_DS1_v2
----

[#verify-security]
== Verify the CoCo pod security restrictions

. Check that logs are disabled
+
[source,sh,role=execute]
----
oc logs pods/hello-openshift
----
+
And notice how no log is printed. This is because of the default Kata policy disallows that.
+
NOTE: if you used https://github.com/kata-containers/kata-containers/blob/main/src/kata-opa/allow-all-except-exec-process.rego[allow-all-except-exec-process.rego, window=blank], logs would be visible. As an exercise (not suitable for production), try to deploy the same `sample-openshift.yaml` with the custom policy, and see how the log works.

. Check that pod exec is disabled
+
[source,sh,role=execute]
----
oc exec -it pods/hello-openshift -- bash
----
+
And notice how an error is returned:
+
[source,texinfo,subs="attributes"]
----
error: Internal error occurred: error executing command in container: cannot enter container 8c0001fb69f7b8e728a3ccc8ad51d362f284f17450765f895db91dce7fc00413, with err rpc error: code = PermissionDenied desc = "ExecProcessRequest is blocked by policy: ": unknown
----

. Check that ssh'ing into the pod Confidential VM is disabled.
+
There are multiple ways to accomplish this, the easies is through the Azure web ui:
+
.. Go to the https://portal.azure.com/#browse/Microsoft.Compute%2FVirtualMachines/subscriptionId/{azure_subscription}[Azure VM web console, window=blank] and login. Insert the email associated with your RHDP account and proceed with login. In the `Subscription` filter, make sure {aro_sandbox_name} is selected. In this example, it is `pool-01-344`.

.. Click on `podvm-hello-openshift-<random_char>` VM, then on the `Connect` side bar menu, and use any of the suggested way to connect with the VM, for example `Serial console` (hidden under `More ways to connect (3)`).

.. If you try with `Serial console`, the following error will appear:
+
[source,texinfo,subs="attributes"]
----
For more information on the Azure Serial Console, see <https://aka.ms/serialconsolelinux>.

Preparing console connection to podvm-hello-openshift-c0311387
  The serial console connection to the VM encountered an error: 'ERR_BAD_REQUEST'

 (403) - Request failed with status code 403
----

[#example-att]
== Attestation example

In this example you will try how the attestation mechanism works, and how easy is to trigger it to get a secret.

IMPORTANT: this pod has a permissive Kata policy that allows the user to see logs and ssh. It is therefore not recommended to run it in production environments.

The `ocp-cc-pod` by itself is simply a CoCo pod that sleeps for an hour and then dies. The particularity is that has a more relaxed Kata policy, so we are able to exec into it and be able to manually trigger attestation to fetch a secret.

. Create `verification-pod.yaml`:
+
[source,sh,role=execute]
----
cat > verification-pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: ocp-cc-pod
  labels:
    app: ocp-cc-pod
  annotations:
    io.katacontainers.config.agent.policy: cGFja2FnZSBhZ2VudF9wb2xpY3kKCmRlZmF1bHQgQWRkQVJQTmVpZ2hib3JzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgQWRkU3dhcFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENsb3NlU3RkaW5SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBDb3B5RmlsZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZVNhbmRib3hSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBEZXN0cm95U2FuZGJveFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEV4ZWNQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR2V0TWV0cmljc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEdldE9PTUV2ZW50UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR3Vlc3REZXRhaWxzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTGlzdEludGVyZmFjZXNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBMaXN0Um91dGVzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTWVtSG90cGx1Z0J5UHJvYmVSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBPbmxpbmVDUFVNZW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBQYXVzZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFB1bGxJbWFnZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFJlYWRTdHJlYW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVTdGFsZVZpcnRpb2ZzU2hhcmVNb3VudHNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXNlZWRSYW5kb21EZXZSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXN1bWVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTZXRHdWVzdERhdGVUaW1lUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2V0UG9saWN5UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2lnbmFsUHJvY2Vzc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFN0YXJ0Q29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhcnRUcmFjaW5nUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhdHNDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTdG9wVHJhY2luZ1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFR0eVdpblJlc2l6ZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUVwaGVtZXJhbE1vdW50c1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUludGVyZmFjZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZVJvdXRlc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFdhaXRQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgV3JpdGVTdHJlYW1SZXF1ZXN0IDo9IHRydWUK
spec:
  runtimeClassName: kata-remote
  containers:
    - name: skr-openshift
      image: registry.access.redhat.com/ubi9/ubi:9.3
      command:
        - sleep
        - "36000"
      securityContext:
        privileged: false
        seccompProfile:
          type: RuntimeDefault
EOF

cat verification-pod.yaml
----
+
Create the pod.
+
[source,sh,role=execute]
----
oc apply -f verification-pod.yaml
----
+
Wait that the pod is created.
+
[source,sh,role=execute]
----
watch oc get pods/hello-openshift
----
+
The pod is ready when the `STATUS` is in `Running`.

. Since it is now allowed, `exec` into the pod.
+
[source,sh,role=execute]
----
oc exec -it ocp-cc-pod -- bash
----

. Fetch `kbsres` keys, more specifically `key1`. This key was added in Trustee when xref:02-configure-trustee.adoc#trustee-key[configuring it]. If you followed the exact instructions, `key1` was configured to contain `Confidential_Secret!`.
+
[source,sh,role=execute]
----
curl http://127.0.0.1:8006/cdh/resource/default/kbsres1/key1 && echo ""
----
+
And as expected, the secret is returned succesfully.
+
[source,texinfo,subs="attributes"]
----
[root@ocp-cc-pod /]# curl http://127.0.0.1:8006/cdh/resource/default/kbsres1/key1 && echo ""
Confidential_Secret!
----
+
IMPORTANT: Notice how the `curl` call is connecting with `http://127.0.0.1`. This is done on purpose, because the CoCo technology is designed to avoid hardcoding any special logic into the pod application. This means that a confidential container doesn't have to know where the Trustee lives, what is its ip, or even care about the attestation report. This is provided in the OSC `AA_KBC_PARAMS` given in the xref:02-configure-osc.adoc#pp-cm[peer-pods configmap]. Such url is then forwarded to the local `Trustee agent` running in side the CoCo Confidential VM automatically, so all the CoCo pod application has to do is communicate **locally** (therefore `http` is enough) with the local `Trustee agent` and ask for the path representing the secret it would like to get, in this case `kbsres1/key1`. The `Trustee agent` will then take care of collecting hardware & software attestation proofs, create an attestation report, enstablish an `https` connection with the remote attester `Trustee operator`, and then perform the attestation process.

. It is also possible to inspect Trustee logs to understand how the process worked.
+
[source,sh,role=execute]
----
POD_NAME=$(oc get pods -l app=kbs -o jsonpath='{.items[0].metadata.name}' -n trustee-operator-system)

oc logs -n trustee-operator-system $POD_NAME
----
+
Expected output (filtering the important logs only):
+
[source,texinfo,subs="attributes"]
----
...
[INFO  api_server::http::attest] Attest API called.
[INFO  attestation_service] AzSnpVtpm Verifier/endorsement check passed.
[INFO  attestation_service] Policy check passed.
...
[INFO  api_server::http::resource] Get resource from kbs:///default/kbsres1/key1
[INFO  api_server::http::resource] Resource access request passes policy check.
[INFO  actix_web::middleware::logger] 10.131.0.9 "GET /kbs/v0/resource/default/kbsres1/key1 HTTP/1.1" 200 514 "-" "attestation-agent-kbs-client/0.1.0" 0.001004
----
+
In this redacted log, we can see how the `AzSnpVtpm` Verifier check passed, how the policy and resource check passed, and eventually the key is sent to the CoCo pod.

[#destroy]
== Destroy the example pods
The pods created in this example section are no different from any other pod, therefore it can be destroyed just as the others (via command line, web ui, etc.). Behind the scenes, the operator will make sure that the created VM will also be completely deallocated.